# Predicci√≥n de riesgo cardiovascular
Aplicar herramientas de explicabilidad de modelos, espec√≠ficamente LIME y SHAP, para analizar y justificar el comportamiento de un modelo de clasificaci√≥n.



# Interpretabilidad de Modelos Predictivos con LIME y SHAP

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-learn](https://img.shields.io/badge/Scikit--learn-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)
![XGBoost](https://img.shields.io/badge/XGBoost-FF6600?style=for-the-badge&logo=xgboost&logoColor=white)
![SHAP](https://img.shields.io/badge/SHAP-29B6F6?style=for-the-badge)
![LIME](https://img.shields.io/badge/LIME-76B041?style=for-the-badge)

## üéØ Objetivo
Aplicar herramientas de explicabilidad de modelos para analizar y justificar el comportamiento de un modelo de clasificaci√≥n, garantizando que las decisiones puedan ser comprendidas por personas no t√©cnicas (clientes, m√©dicos, auditores, usuarios finales).

## üìö Contexto
Este proyecto simula ser parte de un equipo de ciencia de datos que utiliza inteligencia artificial para decisiones cr√≠ticas. La prioridad no es solo construir un modelo preciso, sino tambi√©n explicable, √©tico y responsable.


## üîπ Pasos realizados

### 1. Carga y exploraci√≥n del dataset
- Limpieza m√≠nima de datos: eliminaci√≥n de valores nulos, correcci√≥n de tipos de datos.
- Exploraci√≥n de variables relevantes.
- Identificaci√≥n de posibles variables sensibles (ej. g√©nero, edad, etnia).

### 2. Construcci√≥n del modelo predictivo
- Divisi√≥n del dataset en entrenamiento y prueba.
- Entrenamiento de modelo de clasificaci√≥n (Random Forest, XGBoost o Logistic Regression).
- Evaluaci√≥n del modelo usando m√©tricas: Accuracy y F1-score.

### 3. Aplicaci√≥n de SHAP
- Explicaciones individuales de 3 casos seleccionados.
- Gr√°ficos globales: Summary plot, Bar plot y Waterfall plot.
- Interpretaci√≥n de la influencia de cada variable y patrones emergentes.

### 4. Aplicaci√≥n de LIME
- Explicaciones locales para los mismos 3 casos analizados con SHAP.
- Comparaci√≥n de similitudes y diferencias con SHAP.

### 5. An√°lisis de sesgo y √©tica
- Identificaci√≥n de variables sensibles con posible peso injustificado.
- Propuestas para mitigar sesgos detectados.
- Evaluaci√≥n de riesgos si no se aplicara interpretabilidad.

### 6. Propuesta de mejora
- Ajustes en el modelo o preprocesamiento para decisiones m√°s responsables.
- Consideraciones de cambio de algoritmo, limpieza de variables o nuevas estrategias de feature engineering.

## üìà Resultados esperados
- Modelo de clasificaci√≥n preciso y explicable.
- Casos individuales interpretables mediante LIME y SHAP.
- Reporte sobre posibles sesgos y decisiones √©ticas cr√≠ticas.

## ‚ö° Uso
1. Clonar el repositorio:

git clone https://github.com/usuario/interpretabilidad-modelos.git


